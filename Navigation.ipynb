{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana_Windows_x86_64/Banana.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BananaBrain\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "print(brain_name)\n",
    "print(brain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Actions in the Environment\n",
    "\n",
    "The `play_game` function takes a `use_DQN` boolean parameter that by default is set to False.\n",
    "\n",
    "Here we make use of the Python API to control the agent and receive feedback from the environment. Once this function is called, you will watch the agent's performance.\n",
    "\n",
    "If `use_DQN` is set to `False`, we will select random actions at each time step, otherwise we will use the Deep-QNetwork (which is defined later in this notebook) to pick the best action based on the DQN's approximation over the environment's state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0"
     ]
    }
   ],
   "source": [
    "def play_game(use_DQN = False):\n",
    "    env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "    state = env_info.vector_observations[0]            # get the current state\n",
    "    score = 0\n",
    "    '''\n",
    "    When we play the game using DQN, we don't want to use epsilon-greedy-policy.\n",
    "    Instead, we want to play the game using the strict, greedy-policy.\n",
    "    Only using the greedy-policy we can trully see how the agent performs based no its training.\n",
    "    base\n",
    "    '''\n",
    "    epsilon = 0\n",
    "    while True:\n",
    "        # if we play using DQN, the agent will act based on greedy-policy of the state\n",
    "        # otherwise we use the equiprobable policy to select randomly any of the available actions\n",
    "        action = np.int32(agent.act(state, epsilon)) if use_DQN else np.random.randint(action_size)\n",
    "        \n",
    "        env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "        next_state = env_info.vector_observations[0]   # get the next state\n",
    "        reward = env_info.rewards[0]                   # get the reward\n",
    "        done = env_info.local_done[0]                  # see if episode has finished\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        print(\"\\rScore: {}\".format(score), end=\"\")\n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "# We can now call the play_game function with a \"False\" parameter for DQN usage to randomly play the banana game\n",
    "play_game(use_DQN = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training time!\n",
    "\n",
    "Now we will define and train our own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Defining the Neural Network Architecture\n",
    "\n",
    "Well, in the heart of a Deep Q-Learning system beats the Deep Neural network model that will be used to approximate action selection for any given state.\n",
    "\n",
    "Our neural network will be trained using regression approach, however we will use it as a classifier among the available action set based on the state input. This means that the input size will be as large as our state size (37) and the output size will be as large our action set size (4).\n",
    "\n",
    "How it turns that a regression model will be used as a classifier? Well, we have multiple outputs. Each output measures over the same result, the potential total reward. Thus, since we have multiple outputs, all of which measure the same result, the output with the highest score beats the others.\n",
    "\n",
    "Well, this sounds like classification, where the highest label prevails, and indeed it is. The action to select can be considered as our potential label. As a good refinement we can even use softmax at the final layer if we like, as our outcome won't change, since we will still be considering as the topmost action the one with the highest overall score (no matter if that score is value or probability, we still care about the highest).\n",
    "\n",
    "We will be defining two identical neural networks (local and target). These neural networks should be clones. They should have the same architecture and same initial weights (θ).  We define a seed so that both the online and the target networks will be initialized equally with a random weight set (θ).\n",
    "\n",
    "It appears that three fully connected layers of increasing size contributed to faster training, by consuming less episodes to efficiently approximate the best actions for any state input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=128, fc3_units=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, fc3_units)\n",
    "        self.fc4 = nn.Linear(fc3_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Agent\n",
    "\n",
    "The system will utilize a Deep Q-Network that take advantage of the \"Experience Replay\" and the \"Fixed Q-Targets\" optimizations.\n",
    "\n",
    "### 2.1. - The Params\n",
    "\n",
    "After several configurations test, it appears that the following configurations contribute to a faster training, using less episodes to come up with a policy that meets the needs of this specific Reinforcement Learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 5        # how often to update the network\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. - Defining the Replay Buffer\n",
    "\n",
    "To take advantage of the `Experience Replay` optimization we need to create a buffer that will hold the SARS experience tuples in a memory double-ended queue.\n",
    "\n",
    "#### 2.2.1. - `__init__` constructor\n",
    "\n",
    "In the initialization section we will create\n",
    "* a `double-ended queue` with the buffer_size provided to the constructor that will hold the experience tuples.\n",
    "* a `named tuple` to index the stored experiences by their titles\n",
    "\n",
    "#### 2.2.2. - `add` method\n",
    "\n",
    "The add method appends the SARS tuple (given in the parameters) to the memory\n",
    "\n",
    "#### 2.2.3. - `sample` method\n",
    "\n",
    "Returns a set of random tuples from the memory buffer based on the batch_size (the number of random samples to return).\n",
    "\n",
    "#### 2.2.4. - `__len__` method\n",
    "\n",
    "When using len(myAgent) it will return the size of the memory tuples buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states  = torch.from_numpy(np.vstack([e.state  for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. - Defining the Agent class\n",
    "\n",
    "The agent will train the Deep Q-Networks to explore/exploit which will be used to define the policy that solve the RL problem.\n",
    "\n",
    "It will consist of the following methods with their described functionality:\n",
    "\n",
    "\n",
    "#### 2.3.1 - `__init__` constructor method\n",
    "\n",
    "The constructor will take three parameters, these are `state_size`, `action_size` and `seed`\n",
    "\n",
    "1) in the constructor we will initialize `two QNetwork instances` with the same random seed as to be initialized with the same weights.  The first QNetwork instance will be the online network, while the target network will be the to compare over the changes of the online network and synchronize with the online network based on the `UPDATE_EVERY` frequency from the parameters above.\n",
    "\n",
    "2) We will define the `Optimizer`, in our case we will use Adam, with the specified learning rate `LR` as defined from the parameters above.\n",
    "\n",
    "3) Finally, the agent will instantiate a ReplayBuffer to store its experiences.\n",
    "\n",
    "\n",
    "#### 2.3.2. - `step` method\n",
    "\n",
    "This method will add experiences to the buffer.  If our current step matches the amount of steps based on which we wish to perform learning (UPDATE_EVERY), then we will learn based on random SARS tuple samples as long as the memory contains at least as many items as our batch size (128)\n",
    "\n",
    "#### 2.3.3. - `act` method\n",
    "The act method will fetch the existing state, then in evaluation mode we make a forward pass the state to the neural network and get the output with the scores for the available actions. Now according to the epsilon-greedy-policy probabilities we will either select the highest scored action, or select randomly among any of the available actions.\n",
    "\n",
    "#### 2.3.4. - `learn` method\n",
    "Q-Learning (SarsaMax) dictates that the next action we will take into account is the one that always maximizes the reward at the next state based on our policy. Now applying Q-Learning in a Deep-QN...\n",
    "\n",
    "So the `Q_targets_next` will obtain the the highest action for next state, populated by our batch size.\n",
    "\n",
    "We use this to calculate our `Q_targets` based on our existing `rewards`, so we have a complete SARSA now.\n",
    "\n",
    "Ok, so far we have obtained our SARSA and its now time to see how this change compares to our existing network weights configuration so we make any alterations necessary!\n",
    "\n",
    "Now with a forward call to our DQN we can `gather` the state values, which we call `Q_expected`.\n",
    "\n",
    "Great... now its time to measure the error on our Neural Network. To do so, we will use a regression error function, the `MSE (Mean of Squared Errors)`, which is used to quantify the distance between `Q_expected` and and `Q_targets`. This distance is our error.\n",
    "\n",
    "Cool! Now all that is left is utilize this error to actually train the neural network, expecting to get a new smaller-error.\n",
    "\n",
    "* So we zero our gradients\n",
    "* quantify the loss to all weights using backpropagation by using `backward()` call\n",
    "* and finally apply the gradient change to existing weights using the optimizer's `step()` to come up with new weights\n",
    "\n",
    "Ok! Great! We have finished our learning step now!\n",
    "\n",
    "The only thing is that our target network should sync to our local (online) network until the next learning step (whenever it will reoccur based on the UPDATE_EVERY variable).\n",
    "\n",
    "So we will `soft_update` our target network to match our local network.\n",
    "\n",
    "#### 2.3.5. - `soft_update` method\n",
    "Ok, finally we reach the point where the target network will sync to our online local network! That means that learning has concluded, and we sync the networks to prepare for our next learning.\n",
    "\n",
    "The soft update takes a constant τ (tau), which defines by how much the target network will match the weight configuration of the local network. Passing value 1 will result in clones (fully sync), while anything lower than that will reflect slight variations between the old weights to the new ones. Setting a tau of 0 will never update the target network and it will always keep its original initial configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Initializing the agent object\n",
    "\n",
    "Now we have all the tools to initialize the agent which will explore/exploit the banana world!\n",
    "\n",
    "We initialize the agent setting the\n",
    "\n",
    "* `state_size = brain.vector_observation_space_size`, which is `37`\n",
    "* `action_size = brain.vector_action_space_size`, which is `4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size  = brain.vector_observation_space_size, \n",
    "              action_size = brain.vector_action_space_size, \n",
    "              seed = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Interacting with the Environment to learn\n",
    "\n",
    "At this stage, we will put everything together so we can interact with the environment for several episodes, until we come up with a good policy estimation via our DQN network, and eventually solve the RL task.\n",
    "\n",
    "Our DQN method will run for a maximum of 2000 episodes, after which if we have not reached our desired per 100 episodes score, the method will terminate.\n",
    "\n",
    "In a nutshell, at every episode we will\n",
    "* Reset the environment\n",
    "* For each state, loop by getting the current state\n",
    "    * Follow the epsilon-greedy-policy so the agent acts on that state according policy and epsilon\n",
    "    * Get observation and reward\n",
    "    * Use the agent's `step` function as described above to populate the experience buffer and possibly trigger the `learn` function\n",
    "    \n",
    "We keep doing this until our average score over 100 episodes is equal or higher than 13 (which relates to gathering 13 yellow bananas on time).\n",
    "\n",
    "After every 100 episodes we save a `checkpoint` if the average score has increased, but we also overwrite this checkpoint once we reach our goal. Now one might ask, what is the use to save intermediate scores. Well, sometimes you might interrupt this method earlier, and just want to check how your model performs with this \"early\" training (so it is there just for curiosity reasons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.91\n",
      "Episode 200\tAverage Score: 3.62\n",
      "Episode 300\tAverage Score: 5.53\n",
      "Episode 400\tAverage Score: 8.55\n",
      "Episode 500\tAverage Score: 9.93\n",
      "Episode 600\tAverage Score: 12.47\n",
      "Episode 645\tAverage Score: 13.03\n",
      "Environment solved in 645 episodes!\tAverage Score: 13.03\n"
     ]
    }
   ],
   "source": [
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    prv_score = float(\"-inf\")\n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # reset the environment\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        \n",
    "        # examine the state space \n",
    "        state = env_info.vector_observations[0]  \n",
    "        \n",
    "        score = 0\n",
    "        for t in range(max_t):           \n",
    "            action = np.int32(agent.act(state, eps))       # select the epsilon-greedy action for our state\n",
    "            \n",
    "            env_info = env.step(action)[brain_name]        # forward the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            if (prv_score < np.mean(scores_window)):\n",
    "                torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "       \n",
    "        prv_score = np.mean(scores_window)\n",
    "    \n",
    "        if np.mean(scores_window)>=13.0:\n",
    "            \n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "            \n",
    "    return scores\n",
    "\n",
    "scores = dqn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the scores per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loading the configuration\n",
    "\n",
    "We can load a previous configuration of the DQN agent's weight (θ) here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Playing the game!\n",
    "\n",
    "It is now time to use our defined `play_game` function to play the banana game using the defined Deep QNetwork to approximate the best action based on the state input.\n",
    "\n",
    "\n",
    "### 5.1 Play the game using the DQN policy\n",
    "Play the game using DQN and while the window plays the game, watch here the increasing score as we collect yellow bananas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 16.0"
     ]
    }
   ],
   "source": [
    "play_game(use_DQN = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Play the game using the random policy\n",
    "Compare how the same agent performs if it takes random actions instead (thus ignoring the DQN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0"
     ]
    }
   ],
   "source": [
    "play_game(use_DQN = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are all done, we can now close the environment window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Further Refinements\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several refinements to the existing ones that could add up to the performance of our smart DQN agent.\n",
    "\n",
    "Based on our existing setup, it is much likely that playing further with the hyperparameters we might came up with an agent that could learn faster.\n",
    "\n",
    "However, the agent could greatly benefit from additional RL-specific refinements which we could introduce to our existing setup.\n",
    "\n",
    "More specifically:\n",
    "\n",
    "* Prioritized learning could be used so rather than randomly sampling SARS tuples from our experience buffer, we could revisit more frequently the SARS tuples that better relate to rational actions over specific states.\n",
    "* A Double-DQN could be used to address the DQN problem of action value overestimation\n",
    "* The QNetwork pytorch model could be changed to adapt a dueling DQN architecture, which incorporates two streams. One for the state-values function and another for the advantage-values function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
